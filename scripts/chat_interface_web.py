import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import streamlit as st

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
def load_model(model_path):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—É—Ç–∏.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map='auto',
        torch_dtype=torch.float16
    )
    return tokenizer, model

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
def generate_answer(tokenizer, model, question, temperature=0.8, top_k=35, top_p=0.7, max_new_tokens=2048):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏.
    """
    prompt = f"–û—Ç–≤–µ—á–∞–π –Ω–∞ –†–£–°–°–ö–û–ú. {question}<|endoftext|>"
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
    ).to(model.device)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        top_p=top_p,
        top_k=top_k,
        temperature=temperature,
        repetition_penalty=1.1,
        no_repeat_ngram_size=3,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = full_output[len(prompt):].strip()
    
    return answer

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Streamlit –¥–ª—è —á–∞—Ç–∞
def chat_interface():
    """
    –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á–∞—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Streamlit.
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    model_path = "C:/AI/models/fine_tuned_model"  # –û–±–Ω–æ–≤–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏
    st.title("ü§ñ AI –ß–∞—Ç")

    # –ü–æ–ª—è –¥–ª—è –≤–≤–æ–¥–∞ –≤–æ–ø—Ä–æ—Å–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    question = st.text_input("–í–∞—à –≤–æ–ø—Ä–æ—Å:", "")
    st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    temperature = st.sidebar.slider("Temperature", 0.1, 1.5, 0.8, step=0.1)
    top_k = st.sidebar.slider("Top-k", 1, 100, 35, step=1)
    top_p = st.sidebar.slider("Top-p", 0.0, 1.0, 0.7, step=0.05)
    max_new_tokens = st.sidebar.number_input("Max New Tokens", 10, 4096, 2048, step=10)
    generate_button = st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
    if "response" not in st.session_state:
        st.session_state["response"] = []

    if generate_button and question:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
        if "tokenizer" not in st.session_state or "model" not in st.session_state:
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏..."):
                try:
                    st.session_state["tokenizer"], st.session_state["model"] = load_model(model_path)
                    st.success("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
                    return

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
            try:
                answer = generate_answer(
                    st.session_state["tokenizer"], 
                    st.session_state["model"], 
                    question, 
                    temperature=temperature, 
                    top_k=top_k, 
                    top_p=top_p, 
                    max_new_tokens=max_new_tokens
                )
                st.session_state["response"].append((question, answer))
                st.success("–û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω!")
            except Exception as e:
                st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
    for i, (q, a) in enumerate(st.session_state["response"], 1):
        st.markdown(f"#### –í–æ–ø—Ä–æ—Å {i}")
        st.write(f"{q}")
        st.markdown(f"#### –û—Ç–≤–µ—Ç {i}")
        st.markdown(
            f"<div style='background-color: #5b5b5b; padding: 10px; border-radius: 5px;'>{a}</div>",
            unsafe_allow_html=True
        )
        # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
        st.button("–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç", key=f"copy_button_{i}", on_click=lambda text=a: st.write(f"`{text}`"))

        st.write("---")

# –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
if __name__ == "__main__":
    chat_interface()